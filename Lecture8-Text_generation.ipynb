{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation using Project Gutenberg\n",
    "\n",
    "In this tutorial, we will download a story from Project Gutenberg and use it to feed a text generation model.\n",
    "\n",
    "About Project Gutenberg:\n",
    "\"Project Gutenberg offers over 56,000 free eBooks: Choose among free epub books, free kindle books, download them or read them online. You will find the world's great literature here, especially older works for which copyright has expired. We digitized and diligently proofread them with the help of thousands of volunteers.\"\n",
    "\n",
    "Navigate to https://www.gutenberg.org/ and pick a book with a plain text file of around 200 kb (Alice in Wonderland is a good option: https://www.gutenberg.org/ebooks/11). Download the text as the basis for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop a Small LSTM Recurrent Neural Network\n",
    "In this section we will develop a simple LSTM network to learn sequences of characters from Alice in Wonderland. In the next section we will use this model to generate new sequences of characters.\n",
    "\n",
    "Letâ€™s start off by importing the classes and functions we intend to use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the vocabulary that the network must learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "import re\n",
    "filename = \"./alice.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "#raw_text = re.sub(r':',' ', raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the book is loaded, we must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers.\n",
    "\n",
    "We can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there may be some characters that we could remove to further clean up the dataset that will reduce the vocabulary and may improve the modeling process.\n",
    "\n",
    "Now that the book has been loaded and the mapping prepared, we can summarize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163781\n",
      "Total Vocab:  59\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.\n",
    "\n",
    "In this tutorial we will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. We could just as easily split the data up by sentences and pad the shorter sequences and truncate the longer ones.\n",
    "\n",
    "Each training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  163681\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras.\n",
    "\n",
    "First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n",
    "\n",
    "Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n",
    "\n",
    "Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding. This is so that we can configure the network to predict the probability of each of the different characters in the vocabulary (an easier representation) rather than trying to force it to predict precisely the next character. Each y value is converted into a sparse vector, full of zeros except with a 1 in the column for the letter (integer) that the pattern represents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our LSTM model. Here we define a single hidden LSTM layer with 256 memory units. The network uses dropout with a probability of 20. The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the characters between 0 and 1.\n",
    "\n",
    "The problem is really a single character classification problem with n character classes and as such is defined as optimizing the log loss (cross entropy), here using the ADAM optimization algorithm for speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no test dataset. We are modeling the entire training dataset to learn the probability of each character in a sequence.\n",
    "\n",
    "We are not interested in the most accurate (classification accuracy) model of the training dataset. This would be a model that predicts each character in the training dataset perfectly. Instead we are interested in a generalization of the dataset that minimizes the chosen loss function. We are seeking a balance between generalization and overfitting but short of memorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "163681/163681 [==============================] - 325s 2ms/step - loss: 2.7612\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.76121, saving model to weights-improvement-01-2.7612.hdf5\n",
      "Epoch 2/30\n",
      "163681/163681 [==============================] - 326s 2ms/step - loss: 2.7099\n",
      "\n",
      "Epoch 00002: loss improved from 2.76121 to 2.70990, saving model to weights-improvement-02-2.7099.hdf5\n",
      "Epoch 3/30\n",
      "163681/163681 [==============================] - 325s 2ms/step - loss: 2.6557\n",
      "\n",
      "Epoch 00003: loss improved from 2.70990 to 2.65570, saving model to weights-improvement-03-2.6557.hdf5\n",
      "Epoch 4/30\n",
      "163681/163681 [==============================] - 325s 2ms/step - loss: 2.6133\n",
      "\n",
      "Epoch 00004: loss improved from 2.65570 to 2.61331, saving model to weights-improvement-04-2.6133.hdf5\n",
      "Epoch 5/30\n",
      "163681/163681 [==============================] - 325s 2ms/step - loss: 2.5736\n",
      "\n",
      "Epoch 00005: loss improved from 2.61331 to 2.57358, saving model to weights-improvement-05-2.5736.hdf5\n",
      "Epoch 6/30\n",
      "163681/163681 [==============================] - 326s 2ms/step - loss: 2.5316\n",
      "\n",
      "Epoch 00006: loss improved from 2.57358 to 2.53161, saving model to weights-improvement-06-2.5316.hdf5\n",
      "Epoch 7/30\n",
      "163681/163681 [==============================] - 324s 2ms/step - loss: 2.4909\n",
      "\n",
      "Epoch 00007: loss improved from 2.53161 to 2.49086, saving model to weights-improvement-07-2.4909.hdf5\n",
      "Epoch 8/30\n",
      "163681/163681 [==============================] - 325s 2ms/step - loss: 2.4522\n",
      "\n",
      "Epoch 00008: loss improved from 2.49086 to 2.45224, saving model to weights-improvement-08-2.4522.hdf5\n",
      "Epoch 9/30\n",
      "163681/163681 [==============================] - 324s 2ms/step - loss: 2.4170\n",
      "\n",
      "Epoch 00009: loss improved from 2.45224 to 2.41703, saving model to weights-improvement-09-2.4170.hdf5\n",
      "Epoch 10/30\n",
      "163681/163681 [==============================] - 325s 2ms/step - loss: 2.3837\n",
      "\n",
      "Epoch 00010: loss improved from 2.41703 to 2.38373, saving model to weights-improvement-10-2.3837.hdf5\n",
      "Epoch 11/30\n",
      "163681/163681 [==============================] - 323s 2ms/step - loss: 2.3526\n",
      "\n",
      "Epoch 00011: loss improved from 2.38373 to 2.35255, saving model to weights-improvement-11-2.3526.hdf5\n",
      "Epoch 12/30\n",
      "163681/163681 [==============================] - 324s 2ms/step - loss: 2.3217\n",
      "\n",
      "Epoch 00012: loss improved from 2.35255 to 2.32170, saving model to weights-improvement-12-2.3217.hdf5\n",
      "Epoch 13/30\n",
      "163681/163681 [==============================] - 324s 2ms/step - loss: 2.2929\n",
      "\n",
      "Epoch 00013: loss improved from 2.32170 to 2.29288, saving model to weights-improvement-13-2.2929.hdf5\n",
      "Epoch 14/30\n",
      "163681/163681 [==============================] - 323s 2ms/step - loss: 2.2650\n",
      "\n",
      "Epoch 00014: loss improved from 2.29288 to 2.26497, saving model to weights-improvement-14-2.2650.hdf5\n",
      "Epoch 15/30\n",
      "163681/163681 [==============================] - 324s 2ms/step - loss: 2.2390\n",
      "\n",
      "Epoch 00015: loss improved from 2.26497 to 2.23895, saving model to weights-improvement-15-2.2390.hdf5\n",
      "Epoch 16/30\n",
      "163681/163681 [==============================] - 324s 2ms/step - loss: 2.2109\n",
      "\n",
      "Epoch 00016: loss improved from 2.23895 to 2.21093, saving model to weights-improvement-16-2.2109.hdf5\n",
      "Epoch 17/30\n",
      "163681/163681 [==============================] - 323s 2ms/step - loss: 2.1869\n",
      "\n",
      "Epoch 00017: loss improved from 2.21093 to 2.18688, saving model to weights-improvement-17-2.1869.hdf5\n",
      "Epoch 18/30\n",
      "163681/163681 [==============================] - 324s 2ms/step - loss: 2.1613\n",
      "\n",
      "Epoch 00018: loss improved from 2.18688 to 2.16132, saving model to weights-improvement-18-2.1613.hdf5\n",
      "Epoch 19/30\n",
      "163681/163681 [==============================] - 324s 2ms/step - loss: 2.1415\n",
      "\n",
      "Epoch 00019: loss improved from 2.16132 to 2.14149, saving model to weights-improvement-19-2.1415.hdf5\n",
      "Epoch 20/30\n",
      "163681/163681 [==============================] - 325s 2ms/step - loss: 2.1169\n",
      "\n",
      "Epoch 00020: loss improved from 2.14149 to 2.11691, saving model to weights-improvement-20-2.1169.hdf5\n",
      "Epoch 21/30\n",
      "163681/163681 [==============================] - 327s 2ms/step - loss: 2.0957\n",
      "\n",
      "Epoch 00021: loss improved from 2.11691 to 2.09572, saving model to weights-improvement-21-2.0957.hdf5\n",
      "Epoch 22/30\n",
      "163681/163681 [==============================] - 326s 2ms/step - loss: 2.0782\n",
      "\n",
      "Epoch 00022: loss improved from 2.09572 to 2.07824, saving model to weights-improvement-22-2.0782.hdf5\n",
      "Epoch 23/30\n",
      "163681/163681 [==============================] - 326s 2ms/step - loss: 2.0568\n",
      "\n",
      "Epoch 00023: loss improved from 2.07824 to 2.05682, saving model to weights-improvement-23-2.0568.hdf5\n",
      "Epoch 24/30\n",
      "163681/163681 [==============================] - 327s 2ms/step - loss: 2.0360\n",
      "\n",
      "Epoch 00024: loss improved from 2.05682 to 2.03596, saving model to weights-improvement-24-2.0360.hdf5\n",
      "Epoch 25/30\n",
      "163681/163681 [==============================] - 327s 2ms/step - loss: 2.0191\n",
      "\n",
      "Epoch 00025: loss improved from 2.03596 to 2.01909, saving model to weights-improvement-25-2.0191.hdf5\n",
      "Epoch 26/30\n",
      "163681/163681 [==============================] - 326s 2ms/step - loss: 2.0034\n",
      "\n",
      "Epoch 00026: loss improved from 2.01909 to 2.00345, saving model to weights-improvement-26-2.0034.hdf5\n",
      "Epoch 27/30\n",
      "163681/163681 [==============================] - 325s 2ms/step - loss: 1.9848\n",
      "\n",
      "Epoch 00027: loss improved from 2.00345 to 1.98479, saving model to weights-improvement-27-1.9848.hdf5\n",
      "Epoch 28/30\n",
      "163681/163681 [==============================] - 326s 2ms/step - loss: 1.9720\n",
      "\n",
      "Epoch 00028: loss improved from 1.98479 to 1.97203, saving model to weights-improvement-28-1.9720.hdf5\n",
      "Epoch 29/30\n",
      "163681/163681 [==============================] - 326s 2ms/step - loss: 1.9536\n",
      "\n",
      "Epoch 00029: loss improved from 1.97203 to 1.95359, saving model to weights-improvement-29-1.9536.hdf5\n",
      "Epoch 30/30\n",
      "163681/163681 [==============================] - 325s 2ms/step - loss: 1.9397\n",
      "\n",
      "Epoch 00030: loss improved from 1.95359 to 1.93970, saving model to weights-improvement-30-1.9397.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4278c387f0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(X, y, epochs=30, batch_size=256, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the example, you should have a number of weight checkpoint files in the local directory.\n",
    "\n",
    "You can delete them all except the one with the smallest loss value, eg `weights-improvement-19-1.9435.hdf5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Text with an LSTM Network\n",
    "Generating text using the trained LSTM network is relatively straightforward.\n",
    "\n",
    "Firstly, we load the data and define the network in exactly the same way, except the network weights are loaded from a checkpoint file and the network does not need to be trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"./text_gen_weights_new/weights-improvement-10-2.3837.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, when preparing the mapping of unique characters to integers, we must also create a reverse mapping that we can use to convert the integers back to characters so that we can understand the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to use the Keras LSTM model to make predictions is to first start off with a seed sequence as input, generate the next character then update the seed sequence to add the generated character on the end and trim off the first character. This process is repeated for as long as we want to predict new characters (e.g. a sequence of 1,000 characters in length).\n",
    "\n",
    "We can pick a random input pattern as our seed sequence, then print generated characters as we generate them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  too flustered to tell you--all i know is, something comes at me\n",
      "like a jack-in-the-box, and up i go \"\n",
      " woe toile toe touee toen i mene toe toule toen i men to toe tou do toe mort oo the toite '\n",
      "\n",
      "'io sou toen tou do wou doon ' said the mone turtled  bnd the woile tas io toe toile  and the toene to toe toele to the toele  and the toene tas io toe toile the woile toen the woule the toele to the toele  and the woine tas io toe toile the woile toen the woule toen the woole toen the woole toen the woule the woele toe toele the woele toe woele the woele the woene toe woele the woele the woene tae toe toile the woele the woene the woene the woene the woene the woene the woene the woene the woene tae toe toile the woele the woene the woene the woene the woene the woene the woene the woene the woene tae toe toile the woele the woene the woene the woene the woene the woene the woene the woene the woene tae toe toile the woele the woene the woene the woene the woene the woene the woene the woene the woene tae toe toile the woele the woene the woene the woene the woene the woene the woene the woene\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print( \"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the results shows good language generation? What is done poorly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Try a bigger model\n",
    "\n",
    "We will keep the number of memory units the same at 256, but add a second layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, what about the results shows good language generation? What is done poorly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a word based model\n",
    "\n",
    "Instead of generating language character by character, let's build an word generating model.\n",
    "\n",
    "Use the RNN code from the previous notebook as a guide!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
