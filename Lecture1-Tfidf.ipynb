{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighting words using Tf-Idf\n",
    "\n",
    "We need to start thinking about how to translate collections of texts into quantifiable phenomena.  The easiest way to start is to think about word frequencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I ask you “Do you remember the article about electrons in NY Times?” there’s a better chance you will remember it than if I asked you “Do you remember the article about electrons in the Physics books?”. Here’s why: an article about electrons in NY Times is far less common than in a collection of physics books. It is less likely to stumble upon the “electron” concept in NY Times than in a physics book.\n",
    "\n",
    "Let’s consider now the scenario of a single article. Suppose you read an article and you’re asked to rank the concepts found in the article by importance. The chances are you’ll basically order the concepts by frequency. The reason is simply that important stuff would be mentioned repeatedly because the narrative gravitates around them.\n",
    "\n",
    "Combining the 2 insights, given a term, a document and a collection of documents we can loosely say that:\n",
    "\n",
    "```\n",
    "importance ~ appearances(term, document) / count(documents containing term in collection)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique is called Tf-Idf – Term Frequency – Inverse Document Frequency. Here’s how the measure is defined:\n",
    "\n",
    "```\n",
    "tf = count(word, document) / len(document) – term frequency\n",
    "idf = log( len(collection) / count(document_containing_term, collection) – inverse document frequency )\n",
    "tf-idf = tf * idf – term frequency – inverse document frequency\n",
    "```\n",
    "\n",
    "Let’s test this theory on some data. We’re going to use the Reuters dataset bundles inside NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    " \n",
    "print(len(reuters.fileids()))            # Number of files in the corpus = 10788\n",
    " \n",
    "# Print the categories associated with a file\n",
    "print(reuters.categories('training/999'))      # [u'interest', u'money-fx']\n",
    " \n",
    "# Print the contents of the file\n",
    "print(reuters.raw('test/14829'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\tstacey\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s build a tokenizer that ignores punctuation and stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    " \n",
    "stop_words = stopwords.words('english') + list(punctuation)\n",
    " \n",
    "def tokenize(text):\n",
    "    words = word_tokenize(text)\n",
    "    words = [w.lower() for w in words]\n",
    "    return [w for w in words if w not in stop_words and not w.isdigit()]\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to know all the words inside the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build the vocabulary in one pass\n",
    "vocabulary = set()\n",
    "for file_id in reuters.fileids():\n",
    "    words = tokenize(reuters.raw(file_id))\n",
    "    vocabulary.update(words)\n",
    " \n",
    "vocabulary = list(vocabulary)\n",
    "word_index = {w: idx for idx, w in enumerate(vocabulary)}\n",
    " \n",
    "VOCABULARY_SIZE = len(vocabulary)\n",
    "DOCUMENTS_COUNT = len(reuters.fileids())\n",
    " \n",
    "print(VOCABULARY_SIZE, DOCUMENTS_COUNT)    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s compute the Idf for every word in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idf = defaultdict(lambda: 0)\n",
    "for file_id in reuters.fileids():\n",
    "    words = set(tokenize(reuters.raw(file_id)))\n",
    "    for word in words:\n",
    "        word_idf[word] += 1\n",
    "\n",
    "for word in vocabulary:\n",
    "    word_idf[word] = math.log(DOCUMENTS_COUNT / float(1 + word_idf[word]))\n",
    "\n",
    "print(word_idf['deliberations'])    \n",
    "print(word_idf['committee'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s write, as an exercise, the numpy parallelized version of the Idf computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "word_idf = np.zeros(VOCABULARY_SIZE)\n",
    "for file_id in reuters.fileids():\n",
    "    words = set(tokenize(reuters.raw(file_id)))\n",
    "    indexes = [word_index[word] for word in words]\n",
    "    word_idf[indexes] += 1.0\n",
    "\n",
    "word_idf = np.log(DOCUMENTS_COUNT / (1 + word_idf).astype(float))\n",
    "print(word_idf[word_index['deliberations']])   \n",
    "print(word_idf[word_index['committee']]))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Idf doesn’t depend on the current document but only on the collection we can preprocess the results as we did above. Here’s the code for the final computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tf(word, document):\n",
    "    if isinstance(document, basestring):\n",
    "        document = tokenize(document)\n",
    " \n",
    "    return float(document.count(word)) / len(document)\n",
    " \n",
    "def tf_idf(word, document):\n",
    "    # If not tokenized\n",
    "    if isinstance(document, basestring):\n",
    "        document = tokenize(document)\n",
    " \n",
    "    if word not in word_index:\n",
    "        return .0\n",
    " \n",
    "    return word_tf(word, document) * word_idf[word_index[word]]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print tf_idf('year', reuters.raw('test/14829'))             \n",
    "print tf_idf('following', reuters.raw('test/14829'))       \n",
    "print tf_idf('provided', reuters.raw('test/14829'))        \n",
    "print tf_idf('structural', reuters.raw('test/14829'))        \n",
    "print tf_idf('japanese', reuters.raw('test/14829'))          \n",
    "print tf_idf('downtrend', reuters.raw('test/14829'))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together with a classifier!\n",
    "We're going to go ahead and classify all the documents that are in this dataset (what are all the classes?). This code uses scikit-learns built in functions to classify the data.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, reuters\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "def tokenize(text):\n",
    "    min_length = 3\n",
    "    words = map(lambda word: word.lower(), word_tokenize(text))\n",
    "    words = [word for word in words if word not in cachedStopWords]\n",
    "    tokens = (list(map(lambda token: PorterStemmer().stem(token),words)))\n",
    "    p = re.compile('[a-zA-Z]+');\n",
    "    filtered_tokens = list(filter (lambda token: p.match(token) and len(token) >= min_length,tokens))\n",
    "    return filtered_tokens\n",
    "\n",
    "def represent(documents):\n",
    "    train_docs_id = list(filter(lambda doc: doc.startswith(\"train\"), documents))\n",
    "    test_docs_id = list(filter(lambda doc: doc.startswith(\"test\"), documents))\n",
    "    \n",
    "    train_docs = [reuters.raw(doc_id) for doc_id in train_docs_id]\n",
    "    test_docs = [reuters.raw(doc_id) for doc_id in test_docs_id]\n",
    "    \n",
    "    # Tokenisation\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenize)\n",
    "    \n",
    "    # Learn and transform train documents\n",
    "    vectorised_train_documents = vectorizer.fit_transform(train_docs)\n",
    "    vectorised_test_documents = vectorizer.transform(test_docs)\n",
    "\n",
    "    # Transform multilabel labels\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    train_labels = mlb.fit_transform([reuters.categories(doc_id) for doc_id in train_docs_id]) \n",
    "    test_labels = mlb.transform([reuters.categories(doc_id) for doc_id in test_docs_id])\n",
    "    \n",
    "    return (vectorised_train_documents, train_labels, vectorised_test_documents, test_labels, vectorizer)\n",
    " \n",
    "def train_classifier(train_docs, train_labels):\n",
    "    classifier = OneVsRestClassifier(LinearSVC(random_state=42))\n",
    "    classifier.fit(train_docs, train_labels)\n",
    "    return classifier\n",
    "\n",
    "def evaluate(test_labels, predictions):\n",
    "    precision = precision_score(test_labels, predictions, average='macro')\n",
    "    recall = recall_score(test_labels, predictions, average='macro')\n",
    "\n",
    "    print(\"Precision: {:.4f}, Recall: {:.4f}\".format(precision, recall))\n",
    "    \n",
    "\n",
    "documents = reuters.fileids()\n",
    "train_docs, train_labels, test_docs, test_labels, vectorizer = represent(documents)\n",
    "model = train_classifier(train_docs, train_labels)\n",
    "predictions = model.predict(test_docs)\n",
    "evaluate(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "Can we find which features are important to each class? Try to use some of the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = []\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs.append(feats_df)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "top_feats_by_class(test_docs, test_labels, )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
